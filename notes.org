#+title: BIOSTAT 615 Notes
#+description: Organization File For BIOSTATS 615 @ The University of Michigan, Ann Arbor
#+author: Pete Pritchard
#+LANGUAGE:  en
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
#+LATEX_HEADER: \usepackage{amsmath}

* Course Logistics

** Class Times:

- M/W 8:30am-10am, 1690 SPH I

** Intructor: Hyun Ming Kan

- email: hmkang@umich.edu
- Office Hours:
  - Monday 9:50am - 10:30am, 4623 SPH I Tower
  - Wednesday 9:50am - 10:30am, 4623 SPH I Tower

** GSI: Longhao Pang

- email: lhpang@umich.edu
- Office Hours:
  - Tuesday 3-4:30 M4117 SPH II

* Lecture 1 08-26

** Primarily a couse logistics and syllabus overview.

* Lecture 2 8-28 | Numerical Precision and Relative Error

** Computer representation of numbers

- Computer operations must work with representation of numbers that accomplished by transitors
  - Transitor: a miniature semiconductor that regulates or controls current or voltage flow in addition to amplifying and generating these eletrical signals and acting as as switch/gate for them
  - CPU and RAM
  - Amount of information that can be stored is limited by the amount of size of transitor (storage)
- Two possible states: *off* and *on*
- Number of transitors is finite... can be up to a billion
- To manage overall memory effectively,
  - Restrict the amount of memory that can be allocated to different kinds of numbers
- In the decimal system, numerical values are represented in units or powers of 10
- For a nonnegative integer /k/
  \[
  k = \sum{j=0}{m}a_{j}(10)^{j}, k = (a_{m}a_{m-1} \dots a_{0})_{10}
  \]
  where
- For example,

#+begin_src R
intToBits(127)
intToBits(128)
#+end_src

#+RESULTS:

** Bit and Byte

- *Bit*: the basic unit of information in computing and digital communications
- *Byte*: a unit of storing digital information in memory, consisting of *eight bits*
 op- The connection between *machine memory* and *computer arithmetic*:
  - Bits <-> Transitors, Bytes <-> Blocks of 8 Transitors
- Integer in memory:
  - Binary representation created by:
    - allocating it a block of memory
    - idenifying the individual transitors in the block with a power 2 from its binary representation
    - turning on those transitors that correspond to powers of 2 that have unit (1) coefficients
  - Let's look at *2024* again
    - We need *11* bits to hold integer *2024*
    - *11* bits are needed, so *1* byte is not enough, at least *2* bytes
- More on bytes:
  - A document, image, movie,... how many bytes?
  - *1* byte can hold a letter / character
    - KB = 1 thousand bytes
    - MB = 1 million bytes
    - GB = 1 billion bytes
    - TB = 1 trillion bytes
    - PB = 1 quadrillion bytes

#+begin_src R
s = 2000000000

print(s)

print(s+s)

# What do you expect to be printed?
#+end_src

#+begin_src R
s = 2000000000L

print(s)

print(s+s)

# What do you expect to be printed?
#+end_src

#+begin_src R
noquote(format(.Machine))
#+end_src

- .Machine contains a list of machine precision and limit values
- format(.) displays a list in a tabular format string
- noquote(.)

** R Storage

- Primative data types in R
  1. character: 8 bytes
  2. double: numeric: 8 bytes
  3. integer: 4 bytes
  4. logical: 4 bytes

- The most basic data structure in *R* is an array comprised of one of the primitive data types that is referred to as an /atomic vector/
- Use the *storage.mode* to access the storage mode of a given object
- Use the *object.size* to obtain the memory allocation of an R object
- Machine specific detauls cocnerning storage, etc. are held in the *R* list variable *.machine*

** Floating point representation

- *Q*: Can a computer precisely store an irrational number? Why?
  - *A*: There is a limit to the precision that can be acheived.

** Errors

*** Relative erros in double precision

- Upper bound of relative erross: /(/ 2^{-(m+1)} ) where /m/ is the signficand bits
- For relative precision (double) /m/ = 52
- The relative error of double precision is
  \begin{equation}

  \end{equation}


* Lecture 3 9-04 | Numerical Precision and Relatove Error Cont.

** Quick notes:

- For the mastery assignements there will be no test cases to check like with the learning asssignments
- However, there will be a google collab file that will help you test and debug
- Unlimited submissions just like learning assignment

** Precision errors with quadratic equation

- The challange: (/ | 4b_{2}b_{0} | << b^{2}_{2} /)
- Not accurate in terms in relative error in some cases

** West Algorithm

*** Algorithms (def)

- loose definition: a sequence of well-defined computational steps
- takes a set of values
- produces a set of values

*** Key features of a good algorithm

- Correctness
- Efficiency
- Simplicity

***

* Lecture 4 9-09 | Divide & Conquer

** Time Complexity

- Why is it important?
  - Suppose that I have an algoroithm that takes 1 second for n=1000 sample size
  - If the algorithm has linear time complexity....
    - 1 million, 17 minutes
    - 1 billion, 12 days....
  - If the algorithm has quadratic time complexity...
    - 50000 samples, 42 minutes
    - 1 million samples, 12 days
    - 1 billion, 31000 years
  - In log time complexity...
    - For 1 billion samples, 20 seconds

- Big O notation (upper bounded)
  - This is what we care abou the most... worst case performance
- Big Omega notation (lower bounded)
- Big Theta notation (tight bound)
  - For most of this class, we will use Big Theta notation

** Recursion

- Key components
  - A function that is part of its own definition
  - Terminating condition

- Tower of Hanoi

** Divide and Conquer Algorithm

- Solve a problem recursively, applying three steps at each level of recursion

  1. *Divide*: the problem into a number of subproblems that are smaller instances of the same problem
  2. *Conquer*: the subproblems by solving them recursively. If the subproblem sizes are small enough, however, just solve the problems in a straightforward manner
  3. *Combine*: the solutions to subproblems into the solution for the original problem

*** Insertion Sort

- For k-th step, assume that elements a[1], ... , a[k-1] are already sorted in order.
- Locate a[k] between index 1, ... ,k so that all a[1], ..., a[k] are in order
- Move the focus to k+1-th element and repeat the same step


*** Merge Sort

- Concept behind merge sort is relatively simple
- Keep splitting the array into halves until each subarray is size 1
- Then recursively merge two subarrays at a time
- Time complexity: *O(nlogn)*

Two functions are needed to accomplish this
#+begin_src R
#' merge() : merge two sorted vectors in O(n)
#' @param a - A sorted numeric vector
#' @param b - Another sorted numeric vector
#' @return A sorted vector merging a and b
merge = function(a,b) {
    r = numeric(length(a)+length(b)) # make an empty vector
    i=1; j=1 # i and j are indices for a and b
    for(k in 1:length(r)) {
      ## if b is used up or a[i] < b[j], copy from a
      if ( ( j > length(b) ) || ( i <= length(a) && a[i]<b[j] ) ) {
        r[k] = a[i]
        i = i + 1
      } else {  ## otherwise, copy from b
        r[k] = b[j]
        j = j + 1
      }
    }
    return(r) ## return the merged vector
}

#' mergeSort() : sort an array in O(n log n)
#' @param x A unsorted numeric vector
#' @return A sorted version of x
mergeSort = function(x) {
    if(length(x)>1) {   ## if the element size is greater than 1, keep dividing
        mid = ceiling(length(x)/2)          # find the midpoint
        a = mergeSort(x[1:mid])             # divide - part 1
        b = mergeSort(x[(mid+1):length(x)]) # divide - part 2
        return( merge(a,b) )                # combine the sorted solutions - to me implemented
    } else {  # terminating condition - only 1 element left
        return (x)
    }
}
#+end_src

- most of the lower level languages are using quick sort


* Lecture 5 | Matrix Computation

** Algorithms for matrix computation

- Why do matrices matter?
  - Many statistical models can be represented as matrices
  - Efficeient matric computation can make a difference in the practicality of a method
  - Understanding R implementation of matrix operation can expedite the efficiency by orders of magnitude

- Time compexity is not everything
  - Explains how scalable the algorithm is relative to the increase in the size of input data
  - The *absolute* computational time on an algorithm may depend on the implementation details
  - For example, using a loop inside of R is not usually recommended... slows down implementation by quite a bit

* Lecture 6 | Matrix Computation cont.

** BLAS/LAPACK

- BLAS: implements low-level routines for linear algebra
- LAPACK: implements key algorithms for linear algebra such as matrix decomposition and linear systems solver
- Why are they so fast?

  1. Vectorization
  2. Multithreading
  3. Cache Optimization

- Important to be able to recognize when matrix multiplication is computationally expensive
- ORDER MATTERS!

** Quadratic multiplication

- Consider computing $x'Ax$ where $A=LL'$ (Cholesky decomposition)
- $u=L'x$ can be computed more efficiently than $Ax$
- $x'Ax=u'u$

** Matrix decompisition and solving linear systems

* Single Dimension Optimization Methods

** Root Finding

- Root finding and optimization are closely related problems
- Each type of problem can be converted to the other
  - Root finding -> Optimization: minimize $F(x):{f(x)}^2$
  - Optimization -> Root finding: solve $f(x)=F'(x)=0$
- In both cases, you will find all the solution of the original problem, and possible and some spurious solutions that do not solve the original problem.

*** Bisection Method

- Assume a continuous function $f(x)$
- The intermediate value theorem then tells use that if $f(x)$ is positive at $x=a$ and negative at $x=b$ there has to be a root in between $a$ and $b$
- Basic algorithm structure:
  - Start with the initial points $a_0, b_0$ so that $f(a_0) > 0, f(b_0) < 0$
  - At the midpoint: $x_0 = (a_0 + b_0)/2$, calculate f(x_0)$
  - If $f(x_0)=0$, then it is done; otherwise, one of two things happens:
    - $f(a_0)f(x_0) < 0$, in which case we set $a_1=a_0, b_1=x_0$; or
    - $f(a_0)f(x_0) > 0$, in which case we set $a_1=x_0, b_1=b_0
  - Set $x_1 = (a_1 + b_1) / 2$ and calculate $f(x_1)$
  - Repeat steps 3-4 until $b_n - a_n$ is small enough
- It is impossible to estimate $e_n$ exactly for this method, and ther error may actually grow in steps, but if we interpret $e_n$ as the maximum possible error $e_n=(b_n - a_n)/2$, then $e_{n+1} = (1/2)e_n$
- The method is linearly convergent, with convergence factor $1/2$

*** Fixed Point Iteration

- *NOTE*: this is not a method that is generally used in practice.
- Consider an equivalent equation $f(x)=0$ represented as $g(x)=x. We refer to its solution as a *fixed point* of $g$.
- For $n=0,1,\dots,N$, iteratively evantuate $x_{n+1}=g(x_n$
- Do you think $x_n$ will converge to the root $of f(x) = 0$?
- Example:
  - Target function $f(x)=\cos x -x = 0$
  - Define $g(x) = f(x) + x = \cos x$
  - Then $g(x) = x$ when $f(x)=0$
  - The fixed point algorithm evaluates $x_{n+1} = g(x_n) = \cos x$ for $n = 0, 1,\dots,N$.

*** Newton-Raphson Method

- A special way to construct the fixed point iteration

  \begin{equation}

  g(x) = x - \frac{f(x)}{f'(x)}

  \end{equation}

- *Algorithm:* given an initial guess $x_0$; for $n=0,1,\dots,N$, calculate the tangent to $f$ at $x_n$ and intersect it with the x-axis. This is the next point:

  \begin{equation}

  x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}

  \end{equation}

- This method converges quadratically as long as $f$ has two continuous derivatives.

- Potential problems iwth Newton-Raphson Algo:
  - Huge jumps:
  - Cycles
  - Divergence
  - Requires Derivative

*** The Secant Method

- Discrete version of Newton-Raphson method that does not require derivatives
- Given two initial points $x_0, x_1$: for $n=0,1,\dots,n$ draw a straight line through $(x_n, f(x_n))$ and $(x_1, f(x_1))$. The next point $x_{n+2}$ is the place where this lone intersects the x-axis. We can calculate that:

  \begin{equation*}

  x_{n+2} = \frac{x_n f(x_{n+1}) - x_{n+1} f(x_n)}{f(x_{n+1}) - f(x_n)}

  \end{equation*}

- Suppose $f'(x^*)\neq 0$. If $x_0$ is sifficiently close to $x^*$, then secant method has $e_{n+2} \approx Ce_{n+1}e_n$, thus it converges of order $\frac{\sqrt{5} + 1}{2} \approx 1.618$.


** Minimization Algorithms

Global problems are typically very hard problems to solve. The goal is to find the lowest possible value of the function. Local minimum is a relatively easier problem to solve... finding the smallest value within a finite neighborhood.

Problem goes as follows:

1. Consider a complex function $F(x)$ (e.g. likelihood)
2. Find $x$ which $F(x)$ is a maximum or minimum value
3. Maximicaition and minimzation are equivalent
   + Replace $F(x)$ with $-F(x)$

A function may be costly to evaluate. The derivative of $F$ is not easly to evaluate, so we want to find the minumum of $F$ without the derivative. The strategy is as follows:

1. Find 3 points $a,b,c$ s.t.
   + $a<b<c$
   + $F(b)< F(a)$ and $F(b) < F(c)$
2. Then search for the minimum by
   + Selecting trial point in the intercal $[a,c]$
   + Keep minumum and flanking points

*** Finding a bracketing interval

Goal: startign from two points $a_0$ and $b_0$, find $a,b,c$ s.t. $a<b<c$, $F(b) < F(a)$ and $F(b) < F(c)$. Given these three points, how do we select a new trial point?

We want to minmize the size of the next search interval, which will be either $A$ to $X$ or from $B$ to $C$

Define

\begin{equation*}
        w = \frac{|AB|}{|AC|} = \frac{b-a}{c-a}, z = \frac{BX}{AC} = \frac{x-b}{c-a}
\end{equation*}

Segments will have either length $|BC| = (1-w)|AC|$ or $|AX|=(x+z)|AC|$.

Optimal case:

+ The two possible next search intervals have the same length
+ The proportion of reducing the interval lengths remains the same over iterations

*** Parabolic Approximation

For a root finding problem, linear interpolation likely works better than binary search. For a minimization problem, a parabolic interpolation likely works better than golden search. *Key idea*: approximate the function as a quadratic equation (parabola). Estimate the minimal point based on parabola, and repeat the iteration. An extension of linear interpolation to the minimzation problem. Typically more efficient than golden search, but convergence is not guaranteed.

*** Brent's Method (1973)

*Key idea*: a careful mixture of golden-section search and parabolic interpolation. Predict whether parabolic interpolation would behave poorly. If so, switch to golden section search.

*Features*: combines the reliability of the golden-section search with the efficiency of parabolic interpolation. The most widely used method for single-dimensional minimization. Does not require evaluation of derivatives. Detailed implmentation if quite complicated.


* Multi-dimensional Optimization

What is multi-dimensional oprimization?

- A function takes multiple variabes (or a vector) as parameters.
- The function still returns a sigle-dimensional value.
- The objective is to find a combination of parameters that minimizes the objective function.

Why is multi-dimensional optimization challenging?

- The search space exponentially increases with the number of dimensions.
- A local minimum may not be the global minimum.

** Coordinate descent

The idea behind coordiante descent is simple. If /f/ is a /k/-dimensional function, we can minimize /f/ by succesively minimzing each of the individual dimensions of /f/ in a cyclic fashion, while holding the values of /f/ in other dimensions fixed. In other words, this approach takes an arbitary complex /k/-dimensional problem and reduces it to a collection of /k/ one-dimensional problems.

*Steps:*

1. Start with initial values $x^0 = (x_1^0, \dots, x_n^0)$.
2. Minimize one variable at a time, fixing all other variables.

   \begin{equation*}
        x_i^{k+1} = argmin_y\ F( x_1^{k+1}, \dots,  x_{i-1}^{k+1}, y,  x_{i+}^{k}, \dots,  x_n^{k})
   \end{equation*}

3. Repeat by selecting the next variable to minimize until convergence.

*Key features:*

- One-dimensional optimization may be used.
- Works well for additively seperable funcitons.
- May converge slowly.
- May not converge with specific correlation between dimensions.

Let's take a look at how it works using this function:

\begin{equation*}
        f(x,y) = x^2 + y+2 + xy
\end{equation*}

#+BEGIN_SRC R :results output graphics :file coordinate-descent.png :width 600 :height 400
f <- function(x, y) {
        x^2 + y^2 + x * y
}
n <- 30
xpts <- seq(-1.5, 1.5, len = n)
ypts <- seq(-1.5, 1.5, len = n)
gr <- expand.grid(x = xpts, y = ypts)
feval <- with(gr, matrix(f(x, y), nrow = n, ncol = n))
par(mar = c(5, 4, 1, 1))
contour(xpts, ypts, feval, nlevels = 20, xlab = "x", ylab = "y")
points(-1, -1, pch = 19, cex = 2)
abline(h = -1)
#+END_SRC

#+RESULTS:


** Nelder-Mead method

a.k.a downhill simplex method or amoeba methdod. This method is complicated to understand. This is another minimzation problem: we are trying to make the differences between the calculated and target value as small as possible.

Like other minimzation methods in this section, the problem arises in many situations. Regression, hyper-parameter optimization, machine learning, etc.

*The main gist:*

1. Start with a collection of candidate values, the *Simplex*. The vertices will create a geometric shape with $k+1$ corners, a triangle in $k=2$ dimensions, a tetrahedron in $k=3$ dimensions, so on and so forth. We will consider a triangle in a $k=2$ dimensional space.
2. Compute the value of each of the points, and, as a result, we are left with a current best, current worst, and current second worst position.
3. Each iteration, the algorithm attempts to move the position of the current worst point. The movement is relatiev to the *Centroid* (center of gravity) of the other candidates. To decide what movement to take, the algorithm performs a few tests:

   - *Reflection*: the algorithm tries to move the worst candidate to the other side of the centroid. Then, the /reflected/ candidate is computed (a position that mirrors the worst candidate accross the centroid). If the position is better than the second worst candidate, but not better than the best, the algorithm has made an improvement: replace the worst and repeat the process.
   - *Expansion*: if the /reflected/ candidate is better than the current best, then the algorithm made a move in a promising direction. It then pushes further in the same direction, and compute the /expanded/ candidate. If the /expanded/ candidate is even better, we take it and replace the current worst, otherwise we replace it with the /reflected/ candidate and repeat.
   - If the /refelcted/ candidate is better than our current worst, but not better than our second worst, this is still a promising direction. Attempt a shorter move in the same direction: instead of expanding, a /contracted outside/ candidate is computed, moving less aggressively towards the /reflected/ direction. If that is better than the /reflected/ candiate, we use it and repeat.
   - If it is not an improvement, moving towards the /refelcted/ direction does not help. Instead, we shrink: take the entire simplex and move every point towards teh current best candidate.
   - Lastly, if the /reflected/ candidate is even worse than the current worst, then a move is made in the opposite direction, and computed a /contracted inside/ candidate. If that is an improvement over the current worst, great. Otherwise, there are no more avaialble moves to make and the simplex shrinks.

*** Code

#+BEGIN_SRC R
#' Multi-dimensional Nelder-Mead
#' @param f - Objective function to minimize
#' @param x0 - initial point to start
#' @param tol - relative error
#' @param max_iter - maximum iteration
#' @return A list containing the following attributes:
#'    * xmin : x-coordinate of minima
#'    * fmin : f(xmin)
#'    * convergence : 0 if convergend, 1 otherwise
#'    * iter : number of iterations
Nelder.Mead <- function(f, x0, tol = 1e-10, max_iter = 1000,...){
  d <- length(x0)   # d:dimension of the simplex
  X <- matrix(x0,nrow=d,ncol=d+1)    # set d+1 simplex points
  X[,-(d+1)] <- X[,-(d+1)] + diag(d) # create a simplex
  Y <- apply(X,2,f,...)   # evaluate function at each vertex

  ## initialize key variables as NULL
  idx_max <- NULL; idx_min <- NULL; idx_2ndmax <- NULL ## extremes
  mid_point <- NULL; tru_line <- NULL ## mid-point and tru-line

  ## Function to update the extremes
  update.extremes <- function(){
    ## initialize the worst, 2nd-worst, and the best points
    ## note that global assignment was used to update the variables
    ## outside the function
    if(Y[1] > Y[2]){
      idx_max <<- 1; idx_min <<- 2; idx_2ndmax <<- 2 ## note: global assignment
    } else{
      idx_max <<- 2; idx_2ndmax <<- 1; idx_min <<- 1 ## note: global assignment
    }
    if(d>1){ ## update the worsr, 2nd-worst, and the best
      for(i in 3:(d+1)){
        if(Y[i] <= Y[idx_min]){  ## update the best point
          idx_min <<- i
        } else if(Y[i] > Y[idx_max]){ ## update the worst and 2nd-worst points
          idx_2ndmax <<- idx_max; idx_max <<- i
        } else if(Y[i] > Y[idx_2ndmax]){ ## update the 2nd-worst point
          idx_2ndmax <<- i
        }
      }
    }
  }

  ## Function to update the mid-point and the tru-line
  ## used before performing reflection/expansion/contraction
  update.mid.point <- function(){
    mid_point <<- apply(X[,-idx_max,drop=FALSE],1,mean)
    tru_line <<- X[,idx_max] - mid_point
  }

  ## Function update the next point (reflection, expansion, contraction)
  ## the worst point is replaced with the newly evaluated point if improved
  update.next.point <- function(step_scale){
    next_point <- mid_point + tru_line*step_scale
    Y_next <- f(next_point,...)   ## evaluate the function value
    if(Y_next < Y[idx_max]){      ## if improved over the worst point
      X[,idx_max] <<- next_point  ## replace the worst point
      Y[idx_max] <<- Y_next       ## replace the function value
      return(TRUE)                ## indicate that a point was replaced
    } else{
      return(FALSE)               ## indicate that no point was replaced
    }
  }

  ## Function for multiple contaction
  contract.simplex <- function(){
    X[,-idx_min] <<- 0.5*(X[,-idx_min] + X[,idx_min])
    Y[-idx_min] <<- apply(X[,-idx_min],2,f,...)
  }

  #########################################
  ## the main part of Nelder-Mead algorithm
  #########################################
  convergence = 1
  for(iter in 1:max_iter){
    update.extremes()  ## update worst, 2nd-worst, and the best point

    ## check convergence by comparing the range of the function values
    if(abs(Y[idx_max]-Y[idx_min]) <= tol*(abs(Y[idx_max]) + abs(Y[idx_min]) +tol)){
      convergence = 0  ## converged
      break            ## break the loop
    }
    update.mid.point() ## update mid-point, tru-line

    update.next.point(-1.0)       ## reflection
    if(Y[idx_max] < Y[idx_min]){  ## if reflection generated a new minima
      update.next.point(-2.0)     ## perform expansion
    } else if(Y[idx_max] >= Y[idx_2ndmax]){
      if(!update.next.point(0.5)){  ## perform 1-d contraction
        contract.simplex()          ## if contraction failed, perform multiple contraction
      }
    }
  } ## repeat until convergence

  return(list(xmin=X[,idx_min],  ## return the minima
              fmin=Y[idx_min],   ## return the function value at the minima
              convergence=convergence, ## return convergence indicator
              iter=iter          ## return the number of iterations
        ))
}
#+END_SRC

** Gradient Descent

Gradient descent is an iterative first-order optimization algorithm used to find local minimum/maximum of a fiven function. Very commonly used in machine learning and deep learning to minimize cost/loss function.

What does it mean to be first-oder? It uses gradient.

\begin{equation*}
        x_{n+1} = x_n-\gamme_n \delta F(x_n)
\end{equation*}

The step size, denoted by $\gamma_n$ can be set to a constant, or determined using a line search (i.e. golden section search).

\begin{equation*}
        \gamma_n = \frac{ |(x_n-x_{n-1})^T{\deltaF(X_n) - \deltaF(x_{n-1})}| }{|| \delta F(X_n) - \delta(x_{n-1}) ||^2}
\end{equation*}


* Rcpp

** Microbenchmarking

Accurate timing functions

- Measurement for the performance of a vert small piece of code, something that might take microseconds nanoseconds to run
- Provides very precise timings

** High performance functions with Rcpp

Very easy to connect C++ to R... typical bottlenecks that C++ can address:

- Loops that can not easily vectorized becuase subsequent iterations depend on previous ones.
- Recursive functions, or problems which involve calling fuctions millions of times. The overhead of a function in C++ is much lower than that in R.
- Problems that require advanced data structures and algorithms that R does not provide. Through the standrad template library, C++ has efficient implementations of many important data structures, from ordered maps to double-ended qeues.


* Monte Carlo Methods

** Rejection sampling

- Many distributions are difficult, or even impossible to directly simulate by an inverse form of a CDF
- We need a method that only requires us to know the functional form of the density of interest up to a multiplicative constant; no deep analytical knowledge of the density is necessary
- The key to this method is to use a simpler density from which the simulation is actually done
- /g/: *working density or intstrumental density*
- /f/: *target density*

*** Fundamental Theorem of Simulation

If $f:X \rightarrow R_{+}$ be the target density, Simulating, $X \sim f(x)$, is equivalent to simulating uniformly from the region under the graph of $f$.

\begin{equation}
        (X,U) \sim Unif{(x,u):x\inX, 0<u<f(x)}
\end{equation}


* Assignments:

** Learning Ex. 1:

- `intToBits()` converts to 64-bit by default AND in reversed order. In order to complete this task we need to reverse the order of the vector and remove every even element.
- This can be completed to satisfaction with brute force by following this logic:
  - Check if passed-value is an integer, check if passed-value is numeric (numeric values are 64-bit in base R), check if passed-value is within the bounds of a 32-bit integer value. IF any of these conditions are met, then we want the function to return NA. Otherwise...
  - Convert to bits -> collapse vector into str -> re-vectorize with each int as its own index -> reverse the vector -> remove the even elements -> collapse vector into string again.
- That said, we want to find a more efficient way to solve this.
- R base has only one numeric (float) type which is 64 bit.

** Code

#+BEGIN_SRC R
int2BinaryStr <- function(n) {
  if (!is.numeric(n) || floor(n) != n || n < -2^31 || n > 2^31 - 1) {
    return(NA)
  }

  binary_str <- past(rev(as.integer(intToBits(as.integer(n)))), collaple=""")

  return(binary_str)
}
#+END_SRC

*** Code Breakdown
- r{!is.numeric(n)}: checking if number is not a numeric. Base R treats numeric values as 64-bit
- r{floor(n) != n}: checking if n is not an integer
- r{n < -2^32 || n > 2^31}: checks if n is within the range of 32-bit integer values


** Learning Ex. 2:

** Code
